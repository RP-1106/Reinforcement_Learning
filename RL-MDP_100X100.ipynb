{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8327e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ce5eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions\n",
    "ACTIONS = ['Up', 'Down', 'Left', 'Right']\n",
    "ACTION_IDX = {action: i for i, action in enumerate(ACTIONS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30d8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "alpha = 0.1  \n",
    "gamma = 0.9  \n",
    "epsilon = 0.3  \n",
    "n_episodes = 25\n",
    "grid_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648f283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "green_cell = 100  \n",
    "blue_cell = 50  \n",
    "white_cell = -5  \n",
    "orange_cell = -15\n",
    "black_cell = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ade915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the grid with white cells\n",
    "Grid = np.full((grid_size, grid_size), white_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d8370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set obstacles, penalties, goal, and start positions (fixed for consistency)\n",
    "num_black_cells = random.randint(10, 20)\n",
    "num_orange_cells = random.randint(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ba3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_black_cells):\n",
    "    x, y = random.randint(0, grid_size - 1), random.randint(0, grid_size - 1)\n",
    "    Grid[x, y] = black_cell\n",
    "\n",
    "for _ in range(num_orange_cells):\n",
    "    x, y = random.randint(0, grid_size - 1), random.randint(0, grid_size - 1)\n",
    "    if Grid[x, y] == white_cell:\n",
    "        Grid[x, y] = orange_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1ce63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_x, green_y = random.randint(0, grid_size - 1), random.randint(0, grid_size - 1)\n",
    "Grid[green_x, green_y] = green_cell\n",
    "\n",
    "blue_x, blue_y = random.randint(0, grid_size - 1), random.randint(0, grid_size - 1)\n",
    "Grid[blue_x, blue_y] = blue_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "323da25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table using defaultdict\n",
    "Q = defaultdict(lambda: np.zeros(len(ACTIONS), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd9cb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state):\n",
    "    x, y = state\n",
    "    return Grid[x, y] if 0 <= x < grid_size and 0 <= y < grid_size else black_cell\n",
    "\n",
    "def is_valid_state(state):\n",
    "    x, y = state\n",
    "    return 0 <= x < grid_size and 0 <= y < grid_size and Grid[x, y] != black_cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "646fafbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d8c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(n_episodes):\n",
    "    state = (blue_x, blue_y)\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(ACTIONS)\n",
    "        else:\n",
    "            action = ACTIONS[np.argmax(Q[state])]\n",
    "\n",
    "        # Next state calculation\n",
    "        x, y = state\n",
    "        moves = {'Up': (-1, 0), 'Down': (1, 0), 'Left': (0, -1), 'Right': (0, 1)}\n",
    "        next_state = (x + moves[action][0], y + moves[action][1])\n",
    "\n",
    "        if not is_valid_state(next_state):\n",
    "            next_state = state\n",
    "        else:\n",
    "            # 80% intended move, 20% other valid moves\n",
    "            if random.random() >= 0.8:\n",
    "                valid_moves = [a for a in ACTIONS if a != action and is_valid_state((x + moves[a][0], y + moves[a][1]))]\n",
    "                next_state = (x + moves[random.choice(valid_moves)][0], y + moves[random.choice(valid_moves)][1]) if valid_moves else state\n",
    "\n",
    "        # Get reward and update Q-value\n",
    "        reward = get_reward(next_state)\n",
    "        total_reward += reward\n",
    "        Q[state][ACTION_IDX[action]] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][ACTION_IDX[action]])\n",
    "\n",
    "        # Transition to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # End if goal state is reached\n",
    "        if Grid[state] == green_cell:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64fb3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy extraction\n",
    "policy = np.full((grid_size, grid_size), -1, dtype=int)\n",
    "for x in range(grid_size):\n",
    "    for y in range(grid_size):\n",
    "        if (x, y) in Q:\n",
    "            policy[x, y] = np.argmax(Q[(x, y)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2041751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy (0: UP, 1: DOWN, 2: LEFT, 3: RIGHT):\n",
      "[[1 2 3 ... 1 2 1]\n",
      " [1 1 2 ... 1 2 1]\n",
      " [3 0 2 ... 1 0 2]\n",
      " ...\n",
      " [3 2 0 ... 2 1 3]\n",
      " [2 3 0 ... 3 3 3]\n",
      " [0 2 1 ... 2 2 3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned Policy (0: UP, 1: DOWN, 2: LEFT, 3: RIGHT):\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485e2771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-369375\n",
      "-116040\n",
      "-57595\n",
      "-82425\n",
      "54900\n",
      "2011190\n",
      "13467830\n",
      "12123650\n",
      "5557845\n",
      "27089710\n",
      "70657870\n",
      "27686400\n",
      "179269190\n",
      "133574500\n",
      "400009785\n",
      "2074602480\n",
      "248633295\n",
      "352035485\n",
      "1604873340\n",
      "169397170\n",
      "1628535855\n",
      "172854550\n",
      "1822820320\n",
      "775288885\n",
      "1692090985\n"
     ]
    }
   ],
   "source": [
    "for i in episode_rewards:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3beff00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
